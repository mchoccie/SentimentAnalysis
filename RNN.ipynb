{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Useful Links used:\n",
        "https://www.tensorflow.org/\n",
        "\n",
        "https://keras.io/\n",
        "\n",
        "https://medium.com/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce#:~:text=Embedding%20layer%20enables%20us%20to,way%20along%20with%20reduced%20dimensions.\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/06/natural-language-processing-sentiment-analysis-using-lstm/\n",
        "\n",
        "https://www.kaggle.com/code/lykin22/twitter-sentiment-analysis-with-naive-bayes-85-acc/notebook\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/12/sentiment-analysis-on-tweets-with-lstm-for-beginners/\n",
        "\n",
        "https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c\n",
        "\n",
        "https://haochen23.github.io/2020/01/nlp-rnn-sentiment.html\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/07/performing-sentiment-analysis-with-naive-bayes-classifier/\n",
        "\n",
        "https://www.kaggle.com/code/prashant268/sentiment-analysis-lstm/notebook\n",
        "\n",
        "https://medium.com/analytics-vidhya/understanding-embedding-layer-in-keras-bbe3ff1327ce#:~:text=Embedding%20layer%20enables%20us%20to,way%20along%20with%20reduced%20dimensions\n",
        "\n",
        "https://medium.com/@karyrs1506/sentiment-analysis-on-tweets-with-lstm-22e3bbf93a61\n",
        "\n",
        "https://lifesaver.codes/answer/how-does-embedding-layer-work-3110\n",
        "\n",
        "https://www.kaggle.com/code/prashant268/sentiment-analysis-lstm/notebook\n",
        "\n",
        "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74\n",
        "\n",
        "https://www.kaggle.com/code/arrogantlymodest/randomised-cv-search-over-keras-neural-network/notebook\n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-stemming-in-natural-language-processing/"
      ],
      "metadata": {
        "id": "H5R2B-2KXpDc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMmNT7FDiogD"
      },
      "outputs": [],
      "source": [
        "#####CODE BLOCK 1\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####CODE BLOCK 2\n",
        "# Connect to Google Drive if needed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE0Vu4eojvJX",
        "outputId": "e98cbc0d-779d-4e71-9a8b-36da573f67eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####CODE BLOCK 3\n",
        "#I downloaded the CSV and called it sentiment.csv in my google drive folder called colab notebooks because the file was too large\n",
        "data_train_df = pd.read_csv('./sentiment.csv', encoding='latin-1', names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"])\n",
        "\n",
        "data_train_df.drop('id', axis=1, inplace=True)\n",
        "data_train_df.drop('date', axis=1, inplace=True)\n",
        "data_train_df.drop('flag', axis=1, inplace=True)\n",
        "data_train_df.drop('user', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "xTEHQ-AqjxQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####CODE BLOCK 4\n",
        "# Import necessary Libraries for preprocessing\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(data_train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A85WlUa0jx3z",
        "outputId": "09f6f346-6a2d-4a92-c17b-48233159d49b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "   target                                               text\n",
            "0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
            "1       0  is upset that he can't update his Facebook by ...\n",
            "2       0  @Kenichan I dived many times for the ball. Man...\n",
            "3       0    my whole body feels itchy and like its on fire \n",
            "4       0  @nationwideclass no, it's not behaving at all....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####CODE BLOCK 5\n",
        "#CODE CITATION ACKNOWLEDGMENT: https://medium.com/@karyrs1506/sentiment-analysis-on-tweets-with-lstm-22e3bbf93a61\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "snowball = SnowballStemmer(language='english')\n",
        "tokenizer = TweetTokenizer(preserve_case=False,\n",
        "                           strip_handles=True,\n",
        "                           reduce_len=True)\n",
        "import re\n",
        "#Extract all the tweets from the dataframe\n",
        "texts = list(data_train_df.loc[:, \"text\"])\n",
        "\n",
        "#Prepare a list of tweets which have been cleaned\n",
        "tweet_list = []\n",
        "i = 0\n",
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "#Loop through every tweet\n",
        "while i < len(texts):\n",
        "  tweet = texts[i]\n",
        "  #Remove hyperlinks and @'s\n",
        "  tweet = re.sub('@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+',' ', tweet)\n",
        "  no_punc_tweet = \"\"\n",
        "  list_chars = []\n",
        "  clean_tweet = []\n",
        "\n",
        "  #Tokenzie the tweet after removing unecessary details in the tweet\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "  #Loop through every token in the tweet making sure to remove stop words and punctuation\n",
        "  for word in tweet_tokens:\n",
        "    if word not in stopwords_english and word not in string.punctuation:\n",
        "      #Get the stem of a word to avoid duplicates\n",
        "      word = snowball.stem(word)\n",
        "      clean_tweet.append(word)\n",
        "\n",
        "  #Rejoin the words together\n",
        "  no_punc_tweet = ' '.join(clean_tweet)\n",
        "  no_punc_tweet = no_punc_tweet.lower()\n",
        "  #Add it to our list of clean tweets\n",
        "  tweet_list.append(no_punc_tweet)\n",
        "\n",
        "  i += 1\n",
        "\n",
        "#Create a new dataframe with cleaned tweets\n",
        "data_train_df['refined'] = tweet_list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vvSFfdrEkj1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####CODE BLOCK 6\n",
        "from statistics import mean\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "#Create a Keras tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "print(tweet_list[0])\n",
        "#Create an vocabulary dictionary from all the words in tweets\n",
        "tokenizer.fit_on_texts(tweet_list)\n",
        "#Transfers each word in a tweet to a sequence of integers represented by the dictionary we created before\n",
        "X = tokenizer.texts_to_sequences(tweet_list)\n",
        "#We don't have very large tweets. We can usually determine the sentiment in the first few words\n",
        "X = pad_sequences(X, maxlen = 15, truncating = 'post', padding= 'post')\n",
        "#We want to set our word embedding_length this will change based on if you use Glove or Word2Vec\n",
        "embedding_len = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PaGgiT6myxI",
        "outputId": "43131a71-cd41-4d97-a459-5f4de1d6d069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "awww bummer shoulda got david carr third day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE CITATION ACKNOWLEDGEMENT: https://www.kaggle.com/code/prashant268/sentiment-analysis-lstm/notebook\n",
        "#####CODE BLOCK 7\n",
        "#Use this code block for word embedding using the W2V embedding\n",
        "documents = [tweet.split() for tweet in tweet_list]\n",
        "embedding_len = 300000\n",
        "import gensim\n",
        "import numpy as np\n",
        "#Train the w2v model\n",
        "word_vector_model = gensim.models.word2vec.Word2Vec(documents,\n",
        "                                            size=100,\n",
        "                                            min_count=10,\n",
        "                                            workers=8)\n",
        "\n",
        "\n",
        "#Creating an embeddings matrix and fill it up based on the word vectors we created from before\n",
        "embeddings_matrix = np.zeros((embedding_len, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word in word_vector_model.wv:\n",
        "    embeddings_matrix[i] = word_vector_model.wv[word]\n",
        "print(embeddings_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7Adh2Cf7LFi",
        "outputId": "0ea7023c-1960-4bc4-97bb-630553957fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26726\n",
            "(300000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CODE CITATION ACKNOWLEDGEMENT https://jovian.ai/kamilpolak/sentiment-analysis-with-glove-and-lstm-a3c64\n",
        "#https://www.kaggle.com/datasets/danielwillgeorge/glove6b100dtxt Obtain the glove file from here\n",
        "#####CODE BLOCK 8\n",
        "#Use this code block if you want to use Glove embedding\n",
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "\n",
        "with open('./glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "embeddings_matrix = np.zeros((embedding_len, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "H0YT5sPvm6AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####CODE BLOCK 9\n",
        "from keras.models import Sequential\n",
        "import json\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional, RNN\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "'''\n",
        "CODE SPEED UP: The following helps us run our code on multiple GPUS to speed up code execution\n",
        "'''\n",
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQT7dqMCnijx",
        "outputId": "1521f314-350d-4e23-df64-d10a82a39ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#####CODE BLOCK 10\n",
        "#CODE CITATION ACKNOWLEDGEMENTS USED THE FOLLOWING LINKS FOR HELP\n",
        "#https://jzhao326.github.io/documents/479_DL_REPORT.pdf\n",
        "#https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
        "#https://www.analyticsvidhya.com/blog/2021/06/tuning-hyperparameters-of-an-artificial-neural-network-leveraging-keras-tuner/\n",
        "\n",
        "#PREVIOUS WORK\n",
        "#https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9362853\n",
        "#https://www.kaggle.com/code/gemyhamed/sentiment-analysis-word-embedding-lstm-cnn/notebook\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "Y = data_train_df['target'].tolist()\n",
        "# create a label encoder\n",
        "encoder = LabelEncoder()\n",
        "# Transform the binary data to a set of 0's and 1's\n",
        "encoder.fit(Y)\n",
        "Y = encoder.transform(Y)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n",
        "# reshape y_train and y_test data\n",
        "Y_train = Y_train.reshape(-1, 1)\n",
        "Y_test = Y_test.reshape(-1, 1)\n",
        "x_val = X_train[-10000:]\n",
        "y_val = Y_train[-10000:]"
      ],
      "metadata": {
        "id": "I9OiEeHhoA2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####CODE BLOCK 11\n",
        "#This is the code we will use for Random Search. We must create a model for it\n",
        "# CODE CITATION ACKNOWLEDGEMENT https://www.kaggle.com/code/arrogantlymodest/randomised-cv-search-over-keras-neural-network/notebook\n",
        "from keras.layers import Input,Conv1D,MaxPooling1D,Dense,GlobalMaxPooling1D, Bidirectional,GlobalMaxPool1D, SimpleRNN\n",
        "from keras.models import Model\n",
        "from keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.optimizers import Adadelta, Adam\n",
        "'''\n",
        "CODE SPEED UP: Using random search rather than grid search to find optimum parameters and using mirrored strategy\n",
        "'''\n",
        "!pip install keras-tuner\n",
        "from tensorflow import keras\n",
        "import keras_tuner as kt\n",
        "def modelBuilder(hp):\n",
        "  with strategy.scope():\n",
        "    model = Sequential()\n",
        "    RNNLayer = hp.Choice('units1', values=[200, 120, 100])\n",
        "    DenseLayer1 = hp.Choice('units2', values=[64, 48, 32])\n",
        "    lr = hp.Choice('learning_rate', values=[0.01, 0.1, 0.001])\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.add(Embedding(embedding_len, 100, input_length = 15, weights=[embeddings_matrix]))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Bidirectional(SimpleRNN(RNNLayer)))\n",
        "    model.add(Dense(DenseLayer1, activation='relu'))\n",
        "    model.add(Dense(1, activation = \"sigmoid\"))\n",
        "    model.compile(loss = 'binary_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "_oCJaYMUvWVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####CODE BLOCK 12\n",
        "#If you want to run random search\n",
        "'''\n",
        "CODE SPEED UP: Using random search rather than grid search to find optimum parameters\n",
        "'''\n",
        "batch_size=512\n",
        "tuner = kt.RandomSearch(\n",
        "    modelBuilder,\n",
        "    objective='accuracy',\n",
        "    max_trials=4,\n",
        "    overwrite=True)\n",
        "earlystop = EarlyStopping(monitor='val_accuracy', min_delta = 0.003, patience=1)\n",
        "tuner.search(X_train, Y_train, epochs = 2, batch_size = batch_size, callbacks=[earlystop], validation_data=(x_val, y_val))\n",
        "params = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(params.get(\"units1\"))\n",
        "print(params.get(\"units2\"))\n",
        "print(params.get(\"learning_rate\"))"
      ],
      "metadata": {
        "id": "RJ4aJoFeUU69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####CODE BLOCK 13\n",
        "#This is where we build the model for evaluation using optimum parameters returned from the Random Search\n",
        "with strategy.scope():\n",
        "  optimizer = Adam(learning_rate = 0.001)\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(embedding_len, 100, input_length = 15, weights=[embeddings_matrix]))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Bidirectional(SimpleRNN(100)))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(1, activation = \"sigmoid\"))\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer=optimizer, metrics = ['accuracy', Precision(), Recall()])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "sXWp7WuovY0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####CODE BLOCK 14\n",
        "#If you want to actually run the model\n",
        "'''\n",
        "CODE SPEED UP: Large Batch Size increases the training time of each epoch\n",
        "Also early stopping condition makes sure that the model doesn't overfit\n",
        "'''\n",
        "batch_size=512\n",
        "earlystop = EarlyStopping(monitor='val_accuracy', min_delta = 0.003, patience=1)\n",
        "history = model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, validation_split=0.1, verbose = 1, callbacks=[earlystop])"
      ],
      "metadata": {
        "id": "dQZj8jPYU0AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"./modelRNN.h5\")"
      ],
      "metadata": {
        "id": "Rx8gRfZ134kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training data recroded\n",
        "accuracy=history.history['accuracy']\n",
        "val_accuracy=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "#1157 seconds\n",
        "\n",
        "epochs=range(len(accuracy))"
      ],
      "metadata": {
        "id": "qPOuQon5x-fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtain Precision and Recall\n",
        "scores = model.evaluate(x=X_test, y=Y_test)"
      ],
      "metadata": {
        "id": "TG6vQjwIyKQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "ZADdnPgpMHGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE CITATION ACKNOWLEDGEMENT https://stackoverflow.com/questions/61684542/generating-confusion-matrix-for-keras-model-sentiment-analysis\n",
        "# We want to use sklearn classification report so convert the values to 1's and 0's\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "Y_pred = (model.predict(X_test).ravel()>0.5)+0\n",
        "print(Y_test)"
      ],
      "metadata": {
        "id": "c8FEHjQbjK6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classification Report of results\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "metadata": {
        "id": "QmZpKpDslSVo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}